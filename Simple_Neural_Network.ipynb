{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66B1YXZ2mUVb"
      },
      "source": [
        "This is a simple neural network with one hidden layer and an output layer.\n",
        "The notebook is intended to serve as a skeleton for much bigger networks to be built from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m2Cw703ULvq"
      },
      "source": [
        "# Importing Essential Libraries\n",
        "import numpy as np            # For Numerical Computations\n",
        "import matplotlib.pyplot as plt     # For Visualisation\n",
        "import pandas as pd            # For Data Plotting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsAq9OBqnrBv"
      },
      "source": [
        "Activation Functions and their Derivatives Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rufg90t2mPV2"
      },
      "source": [
        "# Sigmoid/Logistic Activation Function Definition\n",
        "\n",
        "def sigmoid(Z):\n",
        "    return 1/(1 + np.exp(-Z))\n",
        "\n",
        "# Sigmoid/Logistic Function Inverse/Derivative Definition\n",
        "\n",
        "def sigmoid_inv(Z):\n",
        "    return sigmoid(Z) * (1 - sigmoid(Z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJw2mk8WnCB-"
      },
      "source": [
        "# Rectified Linear Unit Activation Function Defninition\n",
        "\n",
        "def relu(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "# Rectified Linear Unit Function Inverse/Derivative Definition\n",
        "\n",
        "def relu_inv(Z):\n",
        "    return Z > 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MV4durSnCFL"
      },
      "source": [
        "# Tanh Activation Function Definition\n",
        "\n",
        "def tanh(Z):\n",
        "    return np.tanh(Z)\n",
        "\n",
        "# Tanh Function Inverse/Derivative Definition\n",
        "\n",
        "def tanh_inv(Z):\n",
        "    return 1. - np.power(np.tanh(Z), 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-EnEf4fo6x0"
      },
      "source": [
        "The following code defines the blueprint of the neural network\n",
        "To keep this simple, the input layer will have n_x (dimension of input matrix X) units, the hidden layer will have n_h (variable) layers, and the output layer will have n_y (dimension of output matrix Y) layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luLj9anLpkyN"
      },
      "source": [
        "def define_layers(X, Y): # Takes in Matrices X and Y (features and labels)\n",
        "\n",
        "    n_x = X.shape[0]\n",
        "    n_h = 16          # Change the value to vary the number of hidden layer units\n",
        "    n_y = Y.shape[0]\n",
        "\n",
        "    return n_x, n_h, n_y      # Basically returns the network architecture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZOdRsR4p9q2"
      },
      "source": [
        "Next code block will initialise the network's weights and biases. Using default random initialisation scaled by 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdZPHxMEqMD9"
      },
      "source": [
        "def init_parameters(n_x, n_h, n_y): # Takes in details of the network architecture\n",
        "\n",
        "    W1 = np.random.rand(n_h, n_x) * 0.01\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    W2 = np.random.rand(n_y, n_h) * 0.01\n",
        "    b2 = np.zeros((n_y, 1))\n",
        "\n",
        "    parameters = {\n",
        "        'W1': W1,\n",
        "        'b1': b1,\n",
        "        'W2': W2,\n",
        "        'b2': b2\n",
        "        }\n",
        "\n",
        "    return parameters # Returns a dictionary of the Weights and Biases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUHP5ugmqcmY"
      },
      "source": [
        "The Feed Forward Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rABLNABzqgFt"
      },
      "source": [
        "def forward_propagate(parameters, X): # Takes in the weights and biases to perform matrix multiplications and activations\n",
        "\n",
        "    W1 = parameters['W1'] # Retrieves the values\n",
        "    b1 = parameters['b1']   # of the parameteres\n",
        "    W2 = parameters['W2']     # from the parameters\n",
        "    b2 = parameters['b2']       # dictionary\n",
        "    \n",
        "    Z1 = W1.dot(X) + b1   # Linear feed forward of the hidden layer\n",
        "    A1 = relu(Z1)            # Using the relu activation to get non linearity\n",
        "    Z2 = W2.dot(A1) + b2        # Linear feed forward of the output layer\n",
        "    A2 = sigmoid(Z2)                # Using the sigmoid activation to get non linearity for the output layer \n",
        "\n",
        "    cache = {\n",
        "        'Z1': Z1,\n",
        "        'A1': A1,\n",
        "        'Z2': Z2,\n",
        "        'A2': A2\n",
        "        } # Saves the values of linear and non-linear functions in to a cache dictionary\n",
        "    \n",
        "    return cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039aL3IXrZU7"
      },
      "source": [
        "The Feed Backward Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47sHNsZYrboI"
      },
      "source": [
        "def backward_propagate(cache, parameters, X, Y):\n",
        "\n",
        "    m = Y.shape[1] # Getting the number of training examples\n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "\n",
        "    Z1 = cache['Z1']\n",
        "    A1 = cache['A1']\n",
        "    Z2 = cache['Z2']\n",
        "    A2 = cache['A2']\n",
        "\n",
        "    dZ2 = A2 - Y            # Derivative of Z2\n",
        "    dW2 = dZ2.dot(A1.T) / m   # Derivative of W2\n",
        "    db2 = np.sum(dZ2) / m       # Derivative of b2\n",
        "    dA1 = W2.T.dot(dZ2)            # Derivative of A1\n",
        "    dZ1 = dA1 * relu_inv(Z1)          # Derivative of Z1\n",
        "    dW1 = dZ1.dot(X.T) / m              # Derivative of W1\n",
        "    db1 = np.sum(dZ1) / m                 # Derivative of b1\n",
        "\n",
        "    # Derivative of x (dx) is basically computing how much the loss changes with a change in the value of x\n",
        "\n",
        "    grads = {\n",
        "        'dW1': dW1,\n",
        "        'db1': db1,\n",
        "        'dW2': dW2,\n",
        "        'db2': db2\n",
        "        }\n",
        "\n",
        "    return grads # Saves the value of the gradients into a dictionary called grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ8fUXBAsDPr"
      },
      "source": [
        "The Loss/Error function. Specifically the logistic loss that compares the predicted value to the actual value. A neural network is meant to learn a set of weights and biases that reduce the value of the loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzy0tdrhsH8Q"
      },
      "source": [
        "def compute_loss(y_hat, y): # y_hat os \n",
        "\n",
        "    m = y.shape[1]\n",
        "    loss = np.multiply(y, np.log(y_hat)) + np.multiply((1 - y), np.log(1 - y_hat)) # Logistic loss formula\n",
        "    cost = - np.sum(loss)\n",
        "    cost /= m\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6oqmXyRspGF"
      },
      "source": [
        "Gradient Descent. This module is the algorithm that actually updates the value of the parameters according to what reduces the cost or loss best"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODzK64Vxs2v8"
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate = 0.05): # learning rate regulates the speed of learning\n",
        "\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    dW1 = grads['dW1']\n",
        "    db1 = grads['db1']\n",
        "    dW2 = grads['dW2']\n",
        "    db2 = grads['db2']\n",
        "\n",
        "    W1 = W1 - (learning_rate * dW1) # The derivative dx\n",
        "    b1 = b1 - (learning_rate * db1)   # is the slope which determines\n",
        "    W2 = W2 - (learning_rate * dW2)     # the steepness and direction of the line\n",
        "    b2 = b2 - (learning_rate * db2)       # therefore telling us how to adjust x properly to reduce cost\n",
        "\n",
        "    parameters = {\n",
        "        'W1': W1,\n",
        "        'b1': b1,\n",
        "        'W2': W2,\n",
        "        'b2': b2\n",
        "        }\n",
        "    \n",
        "    return parameters # A single step of gradient descent overwrites the preexisting parameters with newer parameters that better reduce loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLPHgym_t_jv"
      },
      "source": [
        "Collation of the functions to make a complete neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txh2i2lnuD1E"
      },
      "source": [
        "def neural_network_model(X, Y, num_iterations = 1000):\n",
        "\n",
        "    n_x, n_h, n_y = define_layers(X, Y)\n",
        "    \n",
        "    parameters = init_parameters(n_x, n_h, n_y)\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(0, num_iterations): # num_iterations holds the number of how many gradient descent steps to take\n",
        "        \n",
        "        cache = forward_propagate(parameters, X)\n",
        "                \n",
        "        cost = compute_loss(cache['A2'], Y) # compute the cost after each step\n",
        "        \n",
        "        grads = backward_propagate(cache, parameters, X, Y) # compute gradients\n",
        "        \n",
        "        parameters = update_parameters(parameters, grads) # gradient descent step, adjusting weights to be better\n",
        "        \n",
        "        if i % 10 == 0:\n",
        "            costs.append(cost) # After 10 steps, save the current value of the cost into a array for visualization\n",
        "        if i % 1000 == 0:\n",
        "            print('Cost after', i, 'iterations:', str(cost)) # After 1000 steps, print the current error value to determine if it is reducing (if the model is learning)\n",
        "\n",
        "    plt.plot(costs)                   # plots a graph\n",
        "    plt.xlabel('Number of Iterations')    # of the cost to \n",
        "    plt.ylabel('Cost')                      # for error analysis\n",
        "    plt.show\n",
        "\n",
        "    return parameters # returns the final parameters that the network learns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz8vCEaJv_dE"
      },
      "source": [
        "Module to make predictions using the learned parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIGWB2ZQv-jD"
      },
      "source": [
        "def predict(resulting_parameters, X_test): # Takes in the learned parameters and the input data to predict  \n",
        "\n",
        "    cache = forward_propagate(resulting_parameters, X_test) # Perform forward propagation with the new test data\n",
        "    predictions = cache['A2']                                  # Saves the predictions\n",
        "    prediction = predictions > 0.5                                # Setting the threshold to 0.5\n",
        "    prediction = prediction.astype(int)\n",
        "\n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9LWWZW0w2Fi"
      },
      "source": [
        "Running the next code block runs the entire model\n",
        "This is where you replace X_train and Y_train variables with your training data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz72PATwxJfF"
      },
      "source": [
        "The actual training of your model happens here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNpQxiXRwxAU"
      },
      "source": [
        "resulting_parameters = neural_network_model(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia7kLP3zxD5Z"
      },
      "source": [
        "Calculate the accuracy of your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWPtgKgCxF_9"
      },
      "source": [
        "predictions_on_train_set = predict(resulting_parameters, X_train)\n",
        "accuracy = np.sum((predictions_on_train_set == Y_train).astype(int))/Y_train.size\n",
        "print(\"Accuracy: \", str(accuracy * 100), \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}